# -*- coding: utf-8 -*-
"""Microsoft_Malware_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FtkoyuEZ2_s161Q4I2jVGRrj4V01e-B6
"""

import pandas as pd
from sklearn.model_selection import train_test_split

# Load the dataset
df = pd.read_csv('/content/Microsoft_malware_dataset.csv')

# Display general information
df.info()
df.head()

for col in df.columns:
    unique_values = df[col].nunique()
    missing_percentage = df[col].isnull().sum() * 100 / df.shape[0]
    data_type = df[col].dtype

    print(f"Feature: {col}")
    print(f"  Unique values: {unique_values}")
    print(f"  Percentage of missing values: {missing_percentage:.2f}%")
    print(f"  Type: {data_type}")
    print("\n")

"""We can see several interesting things here:
* PuaMode and Census_ProcessorClass have 99%+ missing values, which means that these columns are useless and should be dropped;

"""

# Identify columns with more than 99% missing values
columns_to_drop = []
for col in df.columns:
    missing_percentage = df[col].isnull().sum() * 100 / df.shape[0]
    if missing_percentage > 99:
        columns_to_drop.append(col)



# Drop the identified columns
df.drop(columns=columns_to_drop, inplace=True)

# Print the columns that were dropped
print(f"Dropped columns: {columns_to_drop}")

# Remove rows with any null values
df.dropna(inplace=True)

# Reset the index if you want a clean index after dropping rows
df.reset_index(drop=True, inplace=True)

# Check the shape of the DataFrame after dropping rows
print("Shape of DataFrame after dropping rows with null values:", df.shape)

df.isnull().sum()

#if exist
df.drop_duplicates(inplace=True)

import matplotlib.pyplot as plt
import seaborn as sns

def plot_categorical_feature(col, top_n=10):
    top_categories = df[col].value_counts().head(top_n)
    plt.figure(figsize=(12, 6))
    sns.countplot(data=df, x=col, order=top_categories.index)
    plt.title(f'Distribution of {col} (Top {top_n} Categories)')
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.xticks(rotation=45)
    plt.show()

# Replace 'df' with your actual DataFrame name and choose a categorical column to plot

column_to_plot = 'Census_IsTouchEnabled'
plot_categorical_feature(column_to_plot)

"""As expected Microsoft has much more computers that touch devices. The rate of infections is lower for touch devices, but not by much.

**Variables with high amount of unique values**
At first I'll have a look at variables with a lot of categories. Then I'll move to more interesting variables with a limited amount of categories.
"""

column_to_plot = 'EngineVersion'
plot_categorical_feature(column_to_plot)

plot_categorical_feature('AppVersion')

plot_categorical_feature('AvSigVersion')

"""This feature has a huge amount of categories, it seems this is a version of an often updated software."""

plot_categorical_feature('AVProductStatesIdentifier')

plot_categorical_feature('AVProductsInstalled')

""" If a computer has an antivirus, it is less likely to be infected. But having two antiviruses has an opposite effect."""

plot_categorical_feature('OsPlatformSubRelease')

"""It is interesting that most computers have Windows 10 (rs*). I suppose Microsoft specifically chose them, so that we would work with modern devices?"""

plot_categorical_feature('OsBuildLab')

import matplotlib.pyplot as plt
import seaborn as sns

outlier_percentage = {}  # To store the percentage of outliers for each column

for col in df.select_dtypes(include=['float64', 'int64']).columns:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1

    # Define the upper and lower bounds for outliers
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Calculate the percentage of outliers
    total_points = df.shape[0]
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
    outlier_percentage[col] = (outliers.shape[0] / total_points) * 100


    # Create a boxplot
    plt.figure(figsize=(8, 4))
    sns.boxplot(x=df[col])
    plt.title(f'Boxplot for {col}')

    #Drop rows with outliers
    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]


# Calculate and print the percentage of outliers for each column
for col, percentage in outlier_percentage.items():
    print(f'Percentage of outliers in {col}: {percentage:.2f}%')

from sklearn.preprocessing import LabelEncoder


# Create an instance of LabelEncoder
labelencoder = LabelEncoder()

# List of categorical columns (object type in your case)
categorical_cols = df.select_dtypes(include=['object']).columns
# Convert all values in categorical columns to string
for column in categorical_cols:
    df[column] = df[column].astype(str)

# Now you can apply LabelEncoder
for column in categorical_cols:
    df[column] = labelencoder.fit_transform(df[column])

df

df.info()

X= df[['Census_PrimaryDiskTotalCapacity','Census_OSBuildRevision','Census_OSBuildNumber','OsBuild','Wdft_IsGamer','Census_IsAlwaysOnAlwaysConnectedCapable'
,'Census_IsTouchEnabled','Census_ThresholdOptIn','Census_FlightRing']]
y = df["HasDetections"]

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold
import pandas as pd
import numpy as np

# Create a Decision Tree Classifier
dt_classifier = DecisionTreeClassifier()

# Define a parameter grid for grid search
param_grid = {
    'max_depth': [None, 10, 20, 30],  # Customize these hyperparameters as needed
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a K-Fold cross-validation object
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Create a GridSearchCV object
grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, scoring='accuracy', cv=kfold)

# Perform grid search and cross-validation
grid_search.fit(X, y)

# Get the best parameters and estimator
best_params = grid_search.best_params_


# Print the best parameters
print("Best Parameters:", best_params)

# Split your dataset into a training set and a testing set using the best hyperparameters
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt


# Correct way to create a DecisionTreeClassifier with specific hyperparameters
clf = DecisionTreeClassifier(max_depth=10, min_samples_leaf=2, min_samples_split=5)

clf.fit(X_train, y_train)

# Predict probabilities
y_probs = clf.predict_proba(X_test)[:, 1]

# Calculate ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_probs)
roc_auc = roc_auc_score(y_test, y_probs)

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc})')
plt.show()

from sklearn.metrics import accuracy_score
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(accuracy*100)

"""Note: That's a significant improvement in accuracy, reaching 73% with the updated list of features in the Decision Tree model. This enhancement suggests that the selection of relevant features has played a crucial role in boosting the model's predictive performance, demonstrating the importance of feature engineering and selection in machine learning."""

import pandas as pd

# Assuming 'X' contains your feature data
# Check for duplicate column names
duplicate_columns = X.columns[X.columns.duplicated()]

if not duplicate_columns.empty:
    print("Duplicate columns found:", duplicate_columns)
else:
    print("No duplicate columns found.")

from sklearn.cluster import KMeans
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import silhouette_score
from sklearn.model_selection import KFold
import pandas as pd
import numpy as np


# Create a K-Means clustering model
kmeans = KMeans()

# Define a parameter grid for grid search
param_grid = {
    'n_clusters': [2, 3, 4, 5],  # Customize the number of clusters
    'init': ['k-means++', 'random'],
    'n_init': [10, 30, 50],
    'max_iter': [300, 500, 1000]
}

# Create a K-Fold cross-validation object
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Create a GridSearchCV object with 'silhouette' as the scoring metric
grid_search = GridSearchCV(estimator=kmeans, param_grid=param_grid, cv=kfold)

# Perform grid search and cross-validation
grid_search.fit(X)



# Get the best parameters and estimator
best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_

# Print the best parameters
print("Best Parameters:", best_params)

# Fit the best estimator on the entire dataset
best_estimator.fit(X)

# Perform cluster assignment (labels_ will contain cluster assignments)
cluster_labels = best_estimator.labels_

# Calculate silhouette score
silhouette_avg = silhouette_score(X, cluster_labels)
print(f"Silhouette Score: {silhouette_avg:.2f}")

"""In our case, with a Silhouette Score of 0.93, it suggests that your clustering is quite good. The majority of data points within each cluster are closer to other data points in the same cluster than they are to data points in neighboring clusters, which is indicative of well-separated clusters."""

import matplotlib.pyplot as plt
import pandas as pd
from sklearn.cluster import KMeans

# Assuming 'X' contains your feature data and 'cluster_labels' contains the cluster assignments

# Create a K-Means clustering model with the optimal parameters
kmeans = KMeans(n_clusters=5, init='k-means++', n_init=10, max_iter=500)

# Fit the model to your data
kmeans.fit(X)

# Get the cluster assignments
cluster_labels = kmeans.labels_

# Create a new DataFrame to store the feature data and cluster labels
df_with_clusters = pd.DataFrame(X, columns=['Census_PrimaryDiskTotalCapacity', 'Census_OSBuildRevision'])

# Add the 'Cluster' column to your DataFrame
df_with_clusters['Cluster'] = cluster_labels

# Get the cluster centroids
centroids = kmeans.cluster_centers_

# Create a scatter plot to visualize the clusters
plt.figure(figsize=(8, 6))
plt.scatter(df_with_clusters['Census_PrimaryDiskTotalCapacity'], df_with_clusters['Census_OSBuildRevision'], c=df_with_clusters['Cluster'], cmap='viridis', s=50, label='Data Points')
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=100, label='Centroids')
plt.xlabel('Census_PrimaryDiskTotalCapacity')
plt.ylabel('Census_OSBuildRevision')
plt.title('K-Means Clustering with Centroids')
plt.colorbar(label='Cluster')
plt.legend()
plt.show()